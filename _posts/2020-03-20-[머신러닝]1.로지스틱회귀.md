---
title: "[머신러닝] 1. 로지스틱 회귀"
date: 2020-03-20 19:27:00 +0800
categories: [연구, 머신러닝]
tags: [머신러닝, 회귀분석, 강의 리뷰]
toc: true
comments: true
use_math: true  	
---



***

> *Intro.*  
> *(내가 알아들은) [머신러닝]이란 사람이 일일이 하기 귀찮거나, 너무 어렵거나, 불가능한 일을*  
> ***기계=컴퓨터**가 대신 해낼 수 있도록, 컴퓨터가 알아듣는 방식으로 **학습**시키는 것이다.*     
> *머신러닝의 큰 그림을 살펴보고 주요 기법들의 원리를 정리해보자.*

***

$$
\def\df#1#2{\frac{#1}{#2}}
\def\si{\sigma}
\def\dt{\cdot}
\def\inf{\infty}
\def\bold#1{\boldsymbol #1}
$$

![분류](\assets\img\머신러닝\classification.png "귀염둥이들...")   

*귀염둥이들...*

# **1. 로지스틱 회귀 *Logistic Regression***

## **Prelude**

![logistic-curve](\assets\img\머신러닝\logistic_regression_01.png)  

<center><cite>귀염둥이들...</cite></center>

가장 흔히 접하는 선형회귀는 우리의 관심 대상인 *target* 변수 $y$(*response*)가   
연속형<sub>*continuous*</sub>으로, 정규분포를 따른다고 가정하지만,  
때로는 $y$가 이산형<sub>*discrete*</sub>인 경우, 그 중 $y$가 $0$과 $1$로만 나뉘는 *binary*인 경우도 있을 것이다.  
*ex)* {성공, 실패}, {호, 불호} 등...   
이 경우, $y$가 성공 확률이 $p=P(Y=1)$인 베르누이 분포를 따른다고 가정하는 것이 더 타당하다.

이런 데이터에 억지로 선형회귀 모형을 fitting하게되면   
이는 좌측 그림과 같이 비효율적인 분석(주어진 정보를 제대로 반영하지 못하는)이 될 것이다.  
또 $x$값의 크기가 일정 부분 이상으로 커지거나 작아지면  
우리가 이후에 예측하게 될 $y$값의 범위는 $[0,1]$을 벗어나게 되므로   
이는 사실상 **무의미한** 모형이 된다.

***

## **로지스틱 회귀**

**로지스틱 회귀 <sub>*Logistic Regression*</sub>**는   
$y$와 $x$들의 관계를 구체적인 함수로 나타낸 모형이라는 측면에서 일반적인 선형회귀와 같지만,   
*target* $y$가 위와 같은 범주형<sub>*categorical*</sub> 데이터일 때 잘 fitting되는 모형이며,

통계학의 주된 관심 분야인 **통계적 추론** - 과거의 데이터**(표본)**로 현상**(모집단)**을 설명하는 것을 넘어  
미래 데이터의 $y$변수($0\;or\;1$)를 예측하되, **'사건의 발생 가능성'**을 예측하는 것에 주 목적이 있으므로  
**분류 *classification***에 있는 기법으로 볼 수 있다. 

> 사실 로지스틱 회귀는 $y$가 다항<sub>*multinomial*</sub>형일 때도,   
> 혹은 순서형<sub>*ordinal*</sub>인 경우에도 사용이 가능하지만  
> 주로 *binary response*에 대한 ***Binomial Logistic Regression***을   
> 그냥 '로지스틱 회귀'라 부르는 것 같다.

결론부터 이야기하면, 기존 선형회귀모형이 $\hat{y}=b_0+b_1x$ 의 형태였다면,   
로지스틱 회귀모형은 $\hat{y}=\df{e^{b_0+b_1x}} {1+e^{b_0+b_1x}}$ 의 형태를 갖는다. 

> 먼저 개념의 이해를 쉽게 하기 위해 (*나를 위해*)  
> $x$ 변수가 $1$개인 '**단순회귀 <sub>*Simple regression*</sub>**' 기준으로 설명을 이어나갈 것이나  
> $x$ 변수가 2개 이상인 '**다중회귀 <sub>*Multiple Regression*</sub>**' 에서도 모두 동일하게 성립한다.

이는 사실 선형회귀모형식을  
부드러운 S자 곡선 형태의 *logistic* 함수(머신러닝 분야에서는 이를 *sigmoid* 함수라 부름),  
 $g(x)=\df {e^x}{1+e^x}=\df {1}{1+e^{-x}}$에 대입한 것과 같다.

*logistic* 함수는 위 우측 그림처럼 입력값으로 어떤 값을 투입하더라도   
그 출력값이 항상 확률의 범위인 $[0,1]$를 벗어나지 않으므로,   
*binary response $y$*를 가지는 데이터에 적합해보인다. 

***

## ***odds ratio* 개념**

잠깐 로지스틱 회귀식을 도출하는 과정을 이해하는데 있어 필요한  
$odds$와 $odds\;ratio$의 개념을 먼저 훑어보자.

만약 **오늘 감기에 걸릴 확률**을 $p$라 한다면,  
자동적으로 감기에 걸리지 않을 확률은 $1-p$가 될 것이다.

*ex)* 오늘 감기에 걸릴 확률 $p=0.1\;\to\;1-p=0.9$ 

여기서 감기에 걸리지 않을(어떤 사건이 발생하지 않을) 확률 대비 감기에 걸릴(발생할) 확률의 비율,   
즉 $\df p {1-p}=\df {0.1} {0.9}=\df 1 9 \simeq 0.11$을 $odds$<sub>**승산**</sub>이라 칭한다. 

이는 확률이 아니며, 확률값을 더 편리하게 사용하기 위해 고안된 개념이라 할 수 있는데,  
사건이 발생할 확률이, 사건이 발생하지 않을 확률에 비해 **몇 배 더 큰가**를 나타내므로  
사건이 발생할 **가능성** 정도로 생각할 수 있고,   
분수로 표기하면 확률과 혼동할 수 있어 소수로 표기하는 것이 국룰인 듯 하다.

$odds$는 $p$가 0에 가까울수록 0에 가까운 작은 값을 가지게 되고,   
반대로 $p$가 1에 가까울수록 $\infty$으로 한없이 커지게 되므로   
$odds$가 클수록 $p$도 크며, 특히 $odds\geq1$일 때 발생 확률이 발생하지 않을 확률보다 큰 것이다.  

이번에는, **내일 감기에 걸릴 확률**이 오늘과 다르다고 생각해보자.  

*ex)*  내일 감기에 걸릴 확률 $p=0.2\;\to\;1-p=0.8$  
그럼 내일의 ***odds***는 $\df 1 4 =0.25$가 될 것이다.

이 때 더 작은 ***odds***를 분모로 하는 두 ***odds***의 비( $\df {내일의\;odds} {오늘의\;odds}$)를 ***odds ratio***라 칭하며,   
위 예시의 오즈비는 $\df{0.25}{0.11}\simeq2.3$배가 될 것이다.  
이는 성공 확률비가 $\df{내일\;감기에\;걸릴\;확률}{오늘\;감기에\;걸릴\;확률}=\df{0.2}{0.1}=2$배인 것과 차이가 있음을 확인할 수 있다.

### **실생활에서 활용되는 예시: 스포츠 복권의 배당률**

*ex)* 월드컵 우승후보인 스페인의 우승배당률이 $9:2$, 한국이 $250:1$이라고 해보자.  
이는 스페인의 우승확률은 $\df{2}{2+9}=\df{2}{11}$ , 한국의 우승확률은 $\df{1}{251}$로 생각한다는 것이다.  
이 때 스페인의 ***odds***는 $\df{2/11}{9/11}=\df{2}{9},\to9:2=4.5:1$이 되어 수익률은 $4.5$배가 된다.  
한국의 ***odds***는 $\df{1/251}{250/251}=\df{1}{250},\to250:1$, 수익률은 $250$배가 된다.   
이에 따라 ***odds ratio***는 $\df{2/9}{1/250}=\df{500}{9}=55.5$배가 되므로,  
스페인의 우승확률을 추정할 때 한국보다 '대략' $55.5$배 높다고 말할 수 있게 되는 것이다.

> 이는 교수님이 들어주신 오즈비의 활용 예시인데, 스포츠에 문외한이라 그런지  
> 배당률이 몇 배라는 말이 무슨 소리인지 아직 잘 모르겠다...

***

### **로지스틱 회귀분석과 오즈비의 관계**

다시 돌아와서,   
로지스틱 회귀분석의 목적은 *target*인 $y$가 $1$일 **확률**을 구하는 것이다.  
그러나 이 때 **확률비** 대신 **오즈비**를 구하게 된다.  
확률비를 직접 구하는 것은 너무 어렵거나, 불가능에 가깝기 때문이다.

***

## **로지스틱 회귀모형의 도출 과정 - 문제 바꿔 풀기**

로지스틱 회귀식의 형태는  
일반 선형회귀식의 장점(변수들을 simple한 선형관계 e.g. $y=ax+b$로 설명)을 그대로 유지하되  
 $y$ 대신 '$y$가 $1$이 되는 확률', 즉 $P(y=1|x)$를 아래의 회귀식으로 표현하는 과정에서 도출된다.  

<br>

\begin{aligned}
P(Y=1|X=x)=\beta_0+\beta_1x​
\end{aligned}

<br>

그러나 이 식은 좌변은 확률의 범위 $[0,1]$을 가지는 반면 우변은 $[-\infty,\infty]$를 가지므로 맞지 않는다.  
그러므로 이를 보정하기 위해 좌변을 $odds$의 형태로 다시 바꾼다.

<br>

\begin{aligned}
\df{P(Y=1|X=x)}{1-P(Y=1|X=x)}=\beta_0+\beta_1x​
\end{aligned}

<br>

이제 좌변은 확률의 범위보다는 더 넓은 $[0,\infty]$를 범위로 갖게 되었으나, 여전히 맞지 않으므로  
추가로 좌변을 로그를 취한 형태로 바꾼다.

<br>

\begin{aligned}
log\left(\df{P(Y=1|X=x)}{1-P(Y=1|X=x)}\right)=\beta_0+\beta_1x​
\end{aligned}

<br>

이제야 좌변과 우변의 범위가 일치하게 되었다. 

이는 또한 로지스틱 회귀를   
**일반화 선형 회귀**<sub>***Generalized Linear Regression***</sub>의 일종으로 볼 수 있는 근거가 되는데,  
$E(Y|X)=\theta$일 때, $g(\theta)=\bold{X} \bold{\beta}$인 $g(\cdot)$로   

> 여기서 $g(\cdot)$를 ***Link Function***이라 부르며, $Y\sim Bern(p)$이므로 $\theta=p$이다.

$logit(p)=log\left(\df p {1-p}\right)$를 사용하므로, 이를 '*logit* 변환'이라 칭한다.   
마지막으로, 이를 다시 $P(Y=1|X=x)$ 기준으로 정리하면 아래와 같다.

<br>

\begin{aligned}
\df{P(Y=1|X=x)}{1-P(Y=1|X=x)} & =exp\\{\beta_0+\beta_1x\\}, \\\\ \\\\ \Rightarrow P(Y=1|X=x)\cdot(1+exp\\{\beta_0+\beta_1x\\}) & =exp\\{\beta_0+\beta_1x\\}, \\\\ \\\\ \therefore P(Y=1|X=x) & =\df{exp\\{\beta_0+\beta_1x\\}}{1+exp\\{\beta_0+\beta_1x\\}}​
\end{aligned}

<br>

$y$가 $1$일 확률을 표현하는 함수는 결국 우리가 위에서 보았던 ***logistic*(*sigmoid*) 함수**가 되며,  
이로 인해 **로지스틱 회귀**라는 이름을 가지게 된 것으로 보인다. 

***

## **Model Fitting - 회귀계수의 통계적 추론**

### **회귀계수의 추정**

로지스틱 회귀모형은 데이터를 가장 잘 설명하는 회귀계수를 추정할 때   
*Least Square Method*를 이용했던 선형회귀와 달리   
***Maximum Likelihood Estimation*** 방법을 이용한다.

간단히 *MLE*를 소개하면 확률밀도함수 *pdf* $f(x)$에 모든 데이터를 대입하여 곱한   
*joint pdf*  $\prod\limits_{i=1}^nf(x_i)$를 모수 $\theta$에 대한 함수로 보는 *Likelihood*, $L(\theta)$에 대해  
이를 *maximize*하는 $\theta$를 **모수의 점 추정량 <sub>*point estimator*</sub>**으로 보는 것이다.  
이는 $L(\theta)$를 $\theta$에 대해 미분하여 $0$이 되는 $\theta$로서 *maximum likelihood estimator*, $\hat{\theta}_{mle}$라 칭한다.

로지스틱 회귀모형에서   
첫째로 우리가 관심있는 모수는 $y=1$일 확률, 곧  
$p=P(Y=1)\;\;where\;\; Y\sim Bern(p)$이다.   

위처럼 베르누이 분포를 따르는 $y$의 *pdf*는 아래와 같다.

<br>

\begin{aligned}
P(Y=y_i)=p^{y_i}(1-p)^{1-y_i}\;\;for\;\;y_i=0,\,1​
\end{aligned}

<br>

이에 따라 $y$의 *Likelihood*는 아래와 같아질 것이다.

<br>

\begin{aligned}
L(p)=\prod\limits_{i=1}^np^{y_i}(1-p)^{1-y_i}​
\end{aligned}

<br>

그러나 지금 우리가 구해야 하는 것은 $y$를 설명하기 위해 추가된 정보라 할 수 있는 $x$들을 바탕으로,  
이를 잘 설명하는 모형의 회귀계수를 구하는 것이므로. 이제부터 우리의 관심 대상인 모수는 $\bold{\beta}$가 된다.  

우리는 앞서 $p=P(Y=1|X=x)=\df{exp\\{\beta_0+\beta_1x\\}}{1+exp\\{\beta_0+\beta_1x\\}}=\theta(\bold{X}\cdot \bold{\beta})$ 임을 구했으므로,   
이를 $L(p)$에 대입한 뒤 다시 $\bold{\beta}$의 함수로 보면, (데이터의 개수가 $n$개일 때)

<br>

\begin{aligned}
L(\bold{\beta})=\prod\limits_{i=1}^n\theta(\bold{X}\cdot \bold{\beta})^{y_i}(1-\theta(\bold{X}\cdot \bold{\beta}))^{1-y_i}​
\end{aligned}

<br>

의 형태로 다시 쓸 수 있다. 

이제 $\hat{\beta}_{mle}$를 구하되,  이를 보다 편하게 구하기 위해   
다음과 같이 *Likelihood*에 로그를 취한 ***Log-likelihood***를 고려할 수 있다.

<br>

\begin{aligned}
ln\,L(\bold{\beta})=\sum\limits_{i=1}^n y_i\,ln\{\theta(\bold{X}\cdot \bold{\beta})\}+\sum\limits_{i=1}^n (1-y_i)\,ln\{1-\theta(\bold{X}\cdot \bold{\beta})\}​
\end{aligned}

<br>

허나 이는 $\bold{\beta}$에 대해 비선형적이기 때문에 선형회귀에서처럼   
이를 최대화시키는 $\bold{\beta}$를 구하는 식이 *Closed Form*으로 도출되지 않는다.

그러므로 $-ln\,L(\bold{\beta})$에 대해 *Gradient Descent*같은 *iterative*한 방식으로  
 $\bold{\beta}$를 추정하는 것이 최선이다. 

만일 이러한 과정에서 모형이 수렴하지 않는다면, 이는 적합한 해가 찾아지지 않는다는 뜻으로  
해당 계수가 중요한 의미를 갖지 않음을 시사한다고 한다.  
또는 다중 공선성 등 모형의 가정을 위배하는 요소가 있을 때도 수렴이 실패할 수 있다고 한다.

### **회귀계수의 검정**

회귀계수를 가설검정할 때,  
귀무가설과 대립가설은 각각  
$H_0:\beta_k=0\;\;\;vs.\;\;\;H_1:\beta_k\neq0$ 이며

검정통계량은 $\chi^2$ 검정통계량을 이용하되  
주로 *Wald test, Likelihood Ratio test, Score test*를 사용한다.

검정 방법들에 대해서는 나중에 수리통계학을 정리하며 포스팅할 기회가 있을 것이다.

***

## **회귀계수의 해석**

추정된 절편값 $b_0$의 해석은 선형회귀에서 절편의 해석과 동일하고 (큰 의미 없다는 것)  
추정된 기울기 $b_1$의 부호는 해당 $x$변수의 *logistic curve*의 경사 방향을 결정하며,   
그 절대적인 크기는 변화율을 의미하는데,   
로지스틱 회귀는 확률비 대신 오즈비를 구한 것이므로 해석에 유의해야한다.

단순 회귀의 예를 들면,  
$y$가 1일 $odds$는 $\df p {1-p}$이고, $p=\df {e^{b_0+b_1x}}{1+e^{b_0+b_1x}}$이므로 $odds=e^{b_0+b_1x}$로 단순해진다.  
$p$와 $1-p$의 분모가 동일하기 때문이다. 

*ex)* 만약 $x$가 '사람의 나이'를 의미하는 변수이고, $b_1=0.3$이라면   
$x=a$에서 $x=a+1$로 나이가 1살 증가한 경우를 생각할 때  
$odds=e^{b_0+0.3a}$에서 $odds=e^{b_0+0.3a+0.3}$로 증가(혹은 감소)한 것이며,  
$odds\;ratio=\df{e^{b_0+0.3a+0.3}} {e^{b_0+0.3a}}=e^{0.3}=1.35$가 된다.  

공교롭게도 $b_1\geq0$일 때 $e^{b_1}\geq1$을 만족하므로 $b_1$의 부호에 따라   
$x$가 1단위 커지면(= 나이가 1살 증가하면),  
$y=1$일 $odds\;ratio$, 즉 **사건이 발생할 가능성**이 $e^{b_1}=e^{0.3}=1.35$배로 증가하거나,   
혹은 $b_1=-0.3$인 경우처럼 $e^{b_1}$이 1보다 작은 경우에는  
 $e^{b_1}=e^{-0.3}=0.74$배 감소한다고 해석하면 되는 것이다.

> $b_1$값 자체로 해석하는 것이 아닌, $e^{b_1}$값으로 해석해야 함을 유의해야 한다. 

만일 기울기 $b_k$가 0이라면 모형은 *flat line*의 형태가 되며,  
 $odds\;ratio$에 변화가 아무런 변화가 없으므로   
이는 해당 $x_k$변수가 $y$값에 미치는 영향, 사건 발생에 미치는 영향이 전혀 없다고 볼 수 있겠다. 

이렇게 다중회귀모형에서도 $x$변수별로 **개별적인 해석**이 가능하다는 것이   
로지스틱 회귀모형의 장점이다. 

***

## **Cutoff: 분류를 결정하는 경계**

위처럼 우리가 앞서 *train* 데이터를 바탕으로 *fitting*한 로지스틱 회귀모형에   
$y$값은 알지 못하고 오직 $x$값들만 알려져 있는 *test* 데이터를 대입하여 나온 $\hat{y}$값을  
$y$가 1일 확률로 여길 수 있게 된다. 

***cutoff***는 *response*의 *classification*을 결정하는 경계선, 혹은 경계면을 의미하는데,  
우리가 얻은 $\hat{y}$값, 즉 $y$가 1일 확률값이 **얼마 이상**이면   
해당 데이터의 $y$값을 1로 예측할 것인지 결정하는 '**분류의 기준**'이 된다.  

가장 기본이 되는 기준은 당연히 $\hat{y}> 0.5$일 것이다.   
이는 곧 $P(Y=1|X=x) > P(Y=0|X=x)$인 *cutoff*가 되며

좌변을 $p$라 할 때, 아래와 같이 정리되므로

<br>

\begin{aligned}
p & > 1-p \\\\ \Rightarrow \df p {1-p} & > 1 \\\\ \Rightarrow ln\,\df p {1-p} & > 0 \\\\ \therefore b_0+b_1x & >0 
\end{aligned}

 <br>

자동적으로 $b_0+b_1x<0$이면 해당 데이터의 $y$를 $0$으로 분류하게 될 것이다.   
따라서 로지스틱 회귀모형의 *cutoff*는 $b_0+b_1x=\bold{X} \bold{\beta}=0$인 **초평면<sub>*hyperplane*</sub>**이 된다.

이는 지금처럼 $x$가 1개인 단순한 경우는 모형의 곡선 중 $\hat{y}$값이 $0.5$가 되는 **한 점**이 될 것이며,  
$x$가 2개인 경우는 모형이 곡면을 이루며 $z$축의 $\hat{y}$값이 $0.5$가 되는 **한 직선**이 초평면이 될 것이다.  
이는 데이터를 둘로 분류하며 본래 모형보다 한 차원 작은 경계 지점이므로 초평면이라 부른다.

물론 상황이나 목적에 따라 경계가 되는 $\hat{y}$값을 상향하거나 하향하여   
보다 보수적이거나, 혹은 널널한 기준을 선택할 수도 있다.  
*ex)* 은행이 고객에게 대출을 해주는 상황이라면 $0.5$보다는 더 까다로운 기준을 적용할 것이다.  

***

## **Reference**

[wikipedia - Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression)

[로지스틱 회귀 - ratsgo's blog]([https://ratsgo.github.io/machine%20learning/2017/04/02/logistic/](https://ratsgo.github.io/machine learning/2017/04/02/logistic/))   



