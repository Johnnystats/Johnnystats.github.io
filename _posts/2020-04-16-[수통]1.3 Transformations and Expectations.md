---
title: "[수리통계학] 1.3 Transformations and Expectations"
date: 2020-04-16 10:24:00 +0800
categories: [수업, 수리통계학]
tags: [수업, 수리통계학, 강의 리뷰]
toc: true
comments: true
use_math: true  	
---

$$
\def\df#1#2{\frac{#1}{#2}}
\def\si{\sigma}
\def\dt{\cdot}
\def\inf{\infty}
\def\F{\mathcal{F}}
\def\G{\mathcal{G}}
\def\R{\mathbb{R}}
\def\lam{\lambda}
\def\om{\omega}
\def\Om{\Omega}
\def\empty{\emptyset}
\def\lto{\longrightarrow}
\def\ds{\displaystyle}
\def\limn{\underset{n\to\inf}{lim}}
\def\E{\exists}
\def\x{\times}
\def\e{\epsilon}
\def\d{\delta}
$$

# **1.3 Transformations and Expectations**

## **1.3.1 Functions of a Random Variable**

$Y=g(X)$ be a new random variable defined by mapping from the sample space $\mathcal{X}\to\mathcal{Y}$,

\begin{align\*}
g(x):\mathcal{X}\to\mathcal{Y}
\end{align\*}

where $\mathcal{X}$ and $\mathcal{Y}$ are the support of random variables $X$ and $Y$ with

\begin{align\*}
\mathcal{X}=\\{x:f_X(x)>0\\}\;\;and\;\;\mathcal{Y}=\\{y:y=g(x)\;for\;some\;x\in\mathcal{X},
\end{align\*}

<br>

**Theorem 1.6** *Let $X$ have cdf $F_X(x)$, let $Y=g(X).$ Then, we have*

(i) *If $g$ is an increasing function on $\mathcal{X},$ $F_Y(y)=F_X(g^{-1}(y))$ for $y\in\mathcal{Y}$.* 

> increasing means $\\{x:\mathcal{X};\;g(x)\leq y\\}=\\{x:g^{-1}\circ g(x)\leq g^{-1}(y)\\}=\\{x:x\leq g^{-1}(y)\\}.$

(ii) *If $g$ is a decreasing function on $\mathcal{X}$ and $X$ is a continuous random variable,  
$F_Y(y)=1-F_X(g^{-1}(y))$ for $y\in\mathcal{Y}$. 

> decreasing means $\\{x:\mathcal{X};\;g(x)\geq y\\}=\\{x:x\geq g^{-1}(y)\\}.$

<br>

**Theorem 1.7** *Let $X$ have pdf $f_X(x)$ and let $Y=g(X)$, where $g$ is a monotone function.  
Let $\mathcal{X}$ and $\mathcal{Y}$ be the support of random variables $X$ and $Y$, respectively.  
Suppose that $f_X(x)$ is continuous on $\mathcal{X}$ and that $g^{-1}(y)$ has a continuous derivative on $\mathcal{Y}$.  
Then the pdf of $Y$ is given by*

\begin{align\*}
f_Y(y)=\begin{cases}
f_X(g^{-1}(y))\left\|\df d {dy} g^{-1}(y)\right\|&if\;y\in\mathcal{Y}\\\\0&otherwise.
\end{cases}​
\end{align\*} 

*(Proof)* Because we have

\begin{align\*}
F_Y(y)=\begin{cases}
1-F_X(g^{-1}(y))&
\rm{if}\;g\;\rm{is}\;\rm{decreasing}\\\\
F_X(g^{-1}(y))&
\rm{if}\;g\;\rm{is}\;\rm{increasing},
\end{cases}​
\end{align\*}

> look at the support of **Theorem 1.6**.

the pdf is computed as

\begin{align\*}
f_Y(y)=\df d {dy}F_Y(y)=\begin{cases}
-f_X(g^{-1}(y))\df d{dy}g^{-1}(y)&
\rm{if}\;g\;\rm{is}\;\rm{decreasing}\\\\
f_X(g^{-1}(y))\df d{dy}g^{-1}(y)&
\rm{if}\;g\;\rm{is}\;\rm{increasing}.\end{cases}​
\end{align\*}

<br>

**Example 1.12** *Let $X$ be a random variable with cdf $F_X$ and Lebesgue pdf $f_X$, and let $Y=X^2$.  
Since $Y^{-1}((-\inf,x])$ is empty if $x<0$ and equals $Y^{-1}([0,x])=X^{-1}([-\sqrt{x},\sqrt{x}])$ if $x\geq 0,$  
the cdf of $Y$ is*

\begin{align\*} 
F_Y(x)&
=P \circ Y^{-1}((-\inf,x])\\\\&
=P \circ X^{-1}([-\sqrt{x},\sqrt{x}])\\\\&
=F_X(\sqrt{x})-F_X(-\sqrt{x})
\end{align\*}

> $P(\rm{empty\;set})=0$ and
> induced measure $P\circ Y^{-1}$, $P\circ X^{-1}$ is CDF.

*if $x\geq 0 $ and $F_Y(x)=0$ if $x<0.$ Clearly, the Lebesgue pdf of $F_Y$ is*

\begin{align\*}
f_Y(x)=\df{1}{2\sqrt{x}}[f_X(\sqrt{x})+f_X(-\sqrt{x})]I_{(0,\inf)}(x).
\end{align\*}

*In particular, if $f_X(x)$ is the pdf of the standard normal distribution $N(0,1)$, then*

\begin{align\*}
f_Y(x)=\df{1}{\sqrt{2\pi x}}exp(-x/2)I_{(0,\inf)}(x),
\end{align\*}

*which is the Lebesgue pdf for the chi-square distribution $\chi^2_1.$*

<br>

**Theorem 1.9** *Let $X$ have continuous cdf $F_X(x)$ and define the random variable $Y$ as  
$Y=F_X(x)$. Then $Y$ is uniformly distributed on $(0,1)$, that is, $P(Y\leq y)=y,\;0<y<1.$  
Conversely, if $Y$ is uniformly distributed over the interval $(0,1),$ then $X=F_X^{-1}(Y)$ has  
cumulative distribution function $F_X(\cdot),$* (random number generation)

> CDF의 성질: random variable ~ $Unif(0,1)$

*(Proof)*

\begin{align\*}
F_Y(y)=P(Y\leq y)&
=P(F_X(X)\leq y)\\\\&
=P(F_X^{-1}[F_X(X)]\leq F_X^{-1}(y))\\\\&
=P(X\leq F_X^{-1}(y))\\\\&
=F_X(F_X^{-1})=y.
\end{align\*}  

and

\begin{align\*}
P(X\leq x)&
=P(F_X^{-1}(Y)\leq x)\\\\&
=P(Y\leq F_X(x))\\\\&
F_X(x)
\end{align\*}

> $Y\sim Unif(0,1)$ 
>
> if we want to generate random number from the CDF, given(known) $F_X(x)$, 
> 1) generate $y^*\sim Unif(0,1)$ and
> 2) take the inverse image $F_X^{-1}(y^*)=$ random number of $X$. 

<br>

## **1.3.2 Expectations**

The expected value, or mean of a random variable $g(X)$ is 

\begin{align\*}
E[g(X)]=
\begin{cases}
\sum\limits_{x\in\mathcal{X}}g(x)f_X(x)&
if\;X\;is\;discrete\\\\
\int_{x\in\mathcal{X}}g(x)f_X(x)dx&
if\;X\;is\;continuous\;(=F_X\;is\;abs\;cts.)
\end{cases}​
\end{align\*} 

<br>

**Theorem 1.10** *Let $X$ be a random variable and let $a,b,$ and $c$ be constants.  
Then for any functions $g_1(x)$ and $g_2(x)$ whose expectations exists,*

(i) $E[ag_1(X)+bg_2(X)+c]=aE[g_1(X)]+bE[g_2(X)]+c$,

(ii) *If $g_1(x)\geq0$ for all $x$, then $E[g_1(X)]\geq 0$,*

(iii) *If $g_1(x)\geq g_2(x)$ for all $x$, then $E[g_1(X)]\geq E[g_2(X)],$*

(iv) *If $a\leq g_1(x) \leq b$ for all $x$, then $a\leq E[g_1(X)]\leq b.$*

<br>

**Example 1.13** *(Binomial mean)*

<br>

**Example 1.14** *(Gamma moment)*

<br>

***



***

## **Reference**







