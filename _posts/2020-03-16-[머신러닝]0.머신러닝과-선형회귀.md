---
title: "[머신러닝] 0. 머신러닝 개괄과 선형회귀"
date: 2020-03-16 23:56:00 +0800
categories: [연구, 머신러닝]
tags: [머신러닝, 선형회귀, 강의 리뷰]
toc: true
comments: true
use_math: true  	
---



***

> *Intro.*  
> *(내가 알아들은) [머신러닝]이란 사람이 일일이 하기 귀찮거나, 너무 어렵거나, 불가능한 일을*  
> ***기계=컴퓨터**가 대신 해낼 수 있도록, 컴퓨터가 알아듣는 방식으로 **학습**시키는 것이다.*     
> *머신러닝의 큰 그림을 살펴보고 주요 기법들의 원리를 정리해보자.*

***

![머신러닝](\assets\img\머신러닝\ML3.png)
$$
\def\df#1#2{\dfrac{#1}{#2}}
\def\si{\sigma}
\def\dt{\cdot}
\def\inf{\infty}
$$

# **0.1 머신러닝 개괄**

## **머신러닝의 정의와 목적**


**Machine Learning**의 가장 유명한 정의는   
미국의 컴퓨터 과학자 *Tom Mitchell*이 내린 것으로, 다음과 같다고 한다.

> "A computer program is said to **learn** from **experience E**  
> with respect to some class of **tasks T** and **performance measure P**,  
> if its performance at tasks in **T**, as measured by **P**, improves with experience **E**.  
> \- *T. Michell (1997)*

포인트를 짚으면 **'경험(Experience)'**, **'과제(Task)'**, **'성능에 대한 지표(Performance Measure)'**  
라는 세 요소가 있고, **"어떤 과제를 달성하는데 있어, 그전에 했던 경험 = 주로 데이터를 통해  
어떤 성능을 향상시키는 것"**이 **머신러닝의 정의**라는 것이다.

그러므로 머신러닝에 대해 어떤 이야기를 하고 싶든 간에  
 ~ *ex)* 고객 데이터 분석, 알고리즘 추천 등...   

우리는 먼저

1) 구체적으로 **과제**를 정의하고,  
2) 이 과제를 잘 설명할 수 있는 **데이터**를 찾고,  
3) **성능 지표**를 최적화하는데 목적을 두어야 한다.   

***

## **각 요소에 대한 구체적인 예시**

### **과제**

먼저 **과제**에 대해 구체적으로 이야기하면, 대표적으로   
$y$값의 종류에 따라 **'분류 classification' / '회귀 regression' / '군집 clustering'**을 들 수 있다. 

1. **분류**: for **discrete** target values  
   
![분류](\assets\img\머신러닝\분류.png)
   
   *ex)* 대표적으로 손으로 쓴 숫자(문자)를 컴퓨터가 인식하게 하는 것이다.  
   사람이 쓴 숫자에 대해 discrete한 선택지(0부터 9까지의 숫자)을 target=$y$변수로 가지는,   
객관식 문제를 맞추는 것(prediction) - input이 어떤 **category**에 들어가는가?
   
2. **회귀**: for **continuous** target value  
   
![회귀](\assets\img\머신러닝\회귀.png)
   
   연속적인 $y$값에 대해 paired data의 **함수를 추정**하는 것이다.  
이를 이용해 연속된 값을 예측하는 것. *ex)* $x$가 $3$일 때, $y$는 무슨 값일까?
   
3. **군집**: for **no** target values

   ![군집](\assets\img\머신러닝\군집.png)

   $y$값이 아예 없는 경우 = 이를 **비지도학습** *unsupervised learning*이라 한다.   
   그저 주어진 자료들이 어떻게 똘똘 뭉쳐있는지 파악하는 것.

   

> *Q.* 어느 그룹에 속하는 지가 곧 새로운 target이 될 수 있으려나? 

***

### **성능 지표**

위에 언급한 대표적인 과제들의 성능을 평가하는 척도로는 **loss function**이 있다.       

1) **분류** 과제에서는 indicator function $I(\hat{y}\neq y)$으로 **오류 점수**($+1, +0$)를 부여하는   
**'0-1 loss function'**을 이용할 수 있고,

2) **회귀** 과제에서는 위처럼 y값의 일치 여부만으로 가르는 것은 비효율적이므로,  
distance를 비교하여 가까운 값은 높은 점수를 주며 먼 값은 낮은 점수를 주는 방식의   
**'$L_2$ loss function'**, $L(y,\hat{y})=\sum \limits_{i=1}^n(y_i-\hat{y}_i)^2$ 을 이용할 수 있다.  
3) **군집** 과제의 경우도 각 군집의 중심을 잡은 뒤   
**'$L_2$ loss function'**로 distance의 총합을 비교할 수 있다.   
총합이 최소가 되는 중심이 최적의 중심이 될 것이다.  

### **경험**

경험은 다른 이야기를 할 필요 없이 데이터를 의미하며, 많으면 많을수록 좋다.  
앞서 말했듯 **labeled data**{e.g. (*pixels*)$\to$(*number*), ($x$)$\to$($y$)}와  
 **unlabeled data**{e.g. ($x_1$,$x_2$)}가 있다. 

***

# **0.2 선형회귀 *Linear Regression***

## **prelude**

통계학 전공자에게 **'회귀분석'**은 **'수리통계학'**과 함께 가장 중심이 되어야만 하는 분야라고 생각한다.  
한 교수님께서 통계학과를 졸업했는데도 회귀분석에 대한 정확한 이해 없이 대충 프로그램만 돌리고,   
결과에 대해 주워들은 해석만 겨우 할 수 있는 사람이 되지 말라고 하셨던 것을 기억한다. *(그게 나야...)*   

동시에 선형회귀는 머신러닝의 가장 기초가 되는 알고리즘이 된다하니   
이번엔 기계를 학습시키는 관점에서 (단순)선형회귀를 어떻게 설명하고 있는지 알아보았다.

## **출발**

여기에 다음과 같은 **paired data **= **labeled data**가 있다고 하자.

$X=(1,2,3)$ $\to$  $Y=(3,5,7)$.   
이는 공부시간~성적, 키~몸무게처럼 어떤 관계가 있는 변수일 것이며,   
input에 따른 response가 미리 주어진 지도학습으로 볼 수 있다. 

그리고 여기 $X=4$일 때, $Y=?$라는 문제가 있다면  
우리는 어려움 없이 $Y=f(X)=2X+1$인 것으로 보이므로, $Y=9$ 겠군! 이라 생각할 것이다.  
이는 우리 사람의 머리로 생각해낸 것이다.

그러나, 만약 데이터의 개수가 엄청 많아지고, 문제가 훨씬 복잡해진다면  
사람의 머리로 푸는 것이 매우 귀찮거나, 어렵거나, 혹은 불가능해질지도 모른다.  
이 때 우리는 컴퓨터에게 대신 이를 해결해달라고 부탁할 수 있으며, 이게 바로 머신러닝이 될 것이다.

위의 문제는 컴퓨터에게 선형회귀 문제로 풀어달라고 요구할 수 있다. 그에 따라  
$X=4$라는 fresh data에 대한 new response, $y$값을 예측할 수 있을 것이다.

## **컴퓨터를 학습시키는 과정**

컴퓨터를 학습시키기 위해, 우리는 아래와 같은 형태의 **가설함수**를 고려할 수 있다.  

\begin{aligned}
H(w,b)=wx+b​ 
\end{aligned}

우리는 $w=2,\; b=1$임을 알고 있지만 모른다고 가정하고,   
가설을 초기화하여 임의의 초기값 $w=1, \;b=0$ 을 설정할 수 있다.  
우리의 목표는 이를 정답인 $w=2,\; b=1$로 만드는 것이다. 

이 때, 현재의 $w,\;b$값이 얼마나 잘못되었는지 판단할 척도가 필요하며, 이를 **Cost**라 칭한다.

일반적으로 선형회귀에서 가장 많이 사용되는 **Cost function**은 곧 **'$L_2$ loss function'**이며, 

이를 수식으로 표현하면 아래와 같다.

 \begin{aligned}
Cost(w,b)=\df 1 n \cdot \sum \limits _{i=1}^{n} (wx_i +b-y_i)^2​
\end{aligned}

Cost function의 형태에 따라,   
이를 가장 작게 만드는 회귀계수 $w$와 $b$를 찾는 방법을 **'최소제곱법'**이라 칭한다. 

> **loss function**의 종류에는 거리($\hat{y}-y$)의 제곱 대신 절대값을 취하는 **'$L_1$ loss function'**도 있으나  
> $L_2$를 이용하는 이유는 제곱이 거리에 대해 더 큰 **penalty**를 부여하므로 더 빠른 학습을 시키는 반면,  
> 절대값을 취하는 것은 컴퓨터가 연산할 때 더 오래걸리기 때문이라고 한다. 

***

이제, 우리가 풀어야 할 문제(정확히는, 컴퓨터에게 풀어달라고 해야 할 문제)는 아래가 된다.   
($n$은 고정, $x$, $y$는 *constant*이므로)

\begin{aligned}
\underset{w,\;b}{argmin}\sum\limits_{i=1}^{n}(wx_i+b-y_i)^2​
\end{aligned}

![note1](\assets\img\머신러닝\n1.jpg)

여기서 절편 $b$를 고정하고 기울기 $w$가 변화한다고 가정하면 ~  
기울기가 변화함에 따라 **Cost**가 기하급수적으로 변화함을 알 수 있고,  
이를 우측 그림과 같은 **2차함수**로 표현할 수 있다.  
그림처럼 현재 상태 $w=1$에서 $w$가 커질수록 **Cost function**의 값이 작아지다가,   
다시 커지는 것을 알 수 있다. 즉, 우측 **convex**한 그래프의 기울기가 0인 지점에서 **Cost**는 최소가 된다.  

이렇게, 반복적으로 $w$를 변화시키며 **Cost**가 최소값이 되는 지점을 찾는 방법을  
**경사하강법 *Gradient Descent***이라고 칭한다.   
말그대로 기울기가 **점점 작아지다 0이 되는 지점**을 찾기 때문이다.

반대로 $w$를 고정시키고 절편 $b$에 대해서도 동일한 접근이 가능함을 알 수 있다.

이는 곧,   
$Cost(w,b)=\df {1} {n} \cdot \sum \limits _{i=1}^{n} (wx_i +b-y_i)^2$을  
$w$, $b$에 대해 각각 편미분한 기울기(Gradient)를 이용하여  
$w$와 $b$ 값을 **update**시키는 과정으로 볼 수 있으며,   

실제로 계산하면  
$w$의 기울기 $=\df \partial {\partial{w}}Cost(w,b)=\sum\limits_{i=1}^{n}(2x_i^2w+2x_ib-2x_iy_i)/n$ ,  
$b$의 기울기 $=\df \partial {\partial{b}}Cost(w,b)=\sum\limits_{i=1}^{n} (2x_iw-2y_i+2b)/n$ 가 된다. 

이제 기존 $w$, $b$에서 각각의 기울기에,   
**learning rate**라 부르는 임의의 상수 $\alpha$($0.0001$ 정도의 매우 작은 값)를   
곱한 것을 뺀 값으로 $w$와 $b$를 **update**해주면 된다.   
이 때 **iteration**(혹은 **epoch**?)은 $1,000$번 이상으로 잡아준다.  

이를 식으로 표현하면 다음과 같다.  

$w\leftarrow w-\alpha\cdot\df \partial {\partial{w}}Cost(w,b)$  
and  
$b\leftarrow b-\alpha\cdot\df \partial {\partial{b}}Cost(w,b)$ 

> 작은 값의 $\alpha$를 곱해주는 이유는 한 번 계수를 **update**할 때 너무 큰 변화를 피하기 위함이다.  

***

이제, training data $X=(1,2,3)$ $\to$  $Y=(3,5,7)$를 이용하여  
이러한 알고리즘을 돌리면 우리의 정답인 $w=2$와 $b=1$로 근사함을 확인할 수 있다. 

이것이 컴퓨터를 학습시켜서 선형회귀를 시키는 가장 쉬운 방법이며,  
사람이 풀 때와의 차이가 있음을 알 수 있다.

사람이 이를 풀 때는 '회귀분석' 시간에 배운대로 각각의 편미분식이 0이 되는,  
2개의 normal equation을 행렬식으로 연립하여 계수를 구해주면 된다.  

그리하면 우리에게 익숙한 $w= \df {Cov(X,Y)} {Var(X)} = \df {S_{xy}} {S_{xx}}$, $b=\bar{y}-w\cdot\bar{x}$가 나온다.

***

### 결론

이게 바로 교양과목 이름에 있던 '컴퓨팅적 사고방식'인가..   
컴퓨터는 계산엔 천재이지만 우리와는 다르게 작동하므로 그를 이해해야 한다.  
이는 단지 언어의 차이로 말이 통하지 않는 외국인을 마주하는 것과는 결이 다른 것 같다. 

