---
title: "[수리통계학] 1.3-2 Moment Generating Function and Characteristic Function"
date: 2020-04-16 17:58:00 +0900
categories: [수업, 수리통계학]
tags: [수업, 수리통계학, 강의 리뷰]
toc: true
comments: true
use_math: true  	
---

$$
\def\df#1#2{\frac{#1}{#2}}
\def\si{\sigma}
\def\dt{\cdot}
\def\inf{\infty}
\def\F{\mathcal{F}}
\def\G{\mathcal{G}}
\def\R{\mathbb{R}}
\def\lam{\lambda}
\def\om{\omega}
\def\Om{\Omega}
\def\empty{\emptyset}
\def\lto{\longrightarrow}
\def\ds{\displaystyle}
\def\limn{\underset{n\to\inf}{lim}}
\def\E{\exists}
\def\x{\times}
\def\e{\epsilon}
\def\d{\delta}
\def\t#1{\textrm{#1}}
\def\sumn{\sum_{n=0}^{\inf}}
$$

## **Prelude**

20세기 초의 분포에 대한 이해는 아래의 moment들로 이루어졌다.  
$E[X]$: mean  
$E[X^2]$: variance  
$E[X^3]$: skewness  
$E[X^4]$: kurtosis. 

두 확률변수 $X$와 $Y$에 대해,  
if $\bar{X}=\bar{Y},\cdots,\bar{X}^4=\bar{Y}^4$ 이면, 두 확률변수의 분포는 같은 분포로 여기는 식이었다.

본 장에서는 분포의 unique한 지문과도 같은  
moment generating function와 보다 상위 개념인 characteristic function을 알아본다.

## **1.3.3 Moments and Moment Generating Functions**

For each positive integer $n$, the $n$-th moment of $X$ is

\begin{align\*}
\mu_n=E[X^n]
\end{align\*}

*and the $n$-th central moment of $X$ is*

\begin{align\*}
\mu_n=E[(X-\mu)^n],
\end{align\*}

*where* $\mu=\mu_1=E[X]$. 

Let $X$ be a random variable with cdf $F_X$. The moment generating function (mgf) of $X$ (or $F_X$),  
denoted by $M_X(t)$, is 

\begin{align\*}
M_X(t)=E(e^{tX}),
\end{align\*}

provided that the expectation exists for $t$ in some neighborhood of $0$. 

> mgf는 $t$의 함수이며,  
> 이러한 Expectation $E(e^{tX})$는 존재하지 않을 수도 있다.

<br>

**Theorem 1.11** *Let $M_X(t)$ be the moment generating function (mgf) of $X$ such that*  

\begin{align\*}
M_X(t)=E[e^{tX}],
\end{align\*}

*provided that the expectation exists for all $\|t\|<\e$, for some $\e>0.$ Then,*

(i) $M_X(t)=\sum\limits_{n=0}^{\inf}t^n\df{\mu_{n+m}}{n!}$ for all $\|t\|<\e.$

(ii) *$M_X(\cdot)$ is infinitely differentiable on $(-\e,\e)$ (around $t=0$) and  
for $m\in\mathbb{N}$, the $m$-th derivative of $M_X(\cdot)$ is*

\begin{align\*}
M_X^{(m)}(t) &= \df{d^n}{dt^n}M_X(t) \\\\&= \sum\limits_{n=0}^{\inf}t^n\df{\mu_{n+m}}{n!}​ \;\t{for all $\|t\|<\e.$}
\end{align\*}

*In particular* 

\begin{align\*}
M_X^{(m)}(0)=0^0\df{\mu_m}{0!}+0+0+\cdots=\mu_m=E[X^m].​
\end{align\*}

*Proof* (i) Since $\left\|\sum\limits_{j=0}^{n}t^j\df{x^j}{j!}\right\|\leq e^{\|tX\|}$, by the DCT, we have

\begin{align\*}
E[e^{tX}] = \ds\int e^{tx}dF_X(x) &= \limn \ds\int \sum_{j=0}^{n}t^j\df{x^j}{j!}dF_X(x) \\\\&= \limn \sum_{j=0}^{n}\df{t^j}{j!}\int x^j dF_X(x) \\\\&= \sum_{j=0}^{\inf} \df {t^j\mu_j}{j!}
\end{align\*}

\begin{align\*}
\because\;\left\|\sum\limits_{j=0}^{n}t^j\df{x^j}{j!}\right\|\leq \sum\limits_{j=0}^{n}\left\|t^j\df{x^j}{j!}\right\|\leq\sum\limits_{j=0}^{\inf}\left\|\df{(tx)^j}{j!}\right\|= e^{\|tX\|}
\end{align\*}

> DCT는 $\F$-measurable function의 sequence의 상한이 있을 때,  
> limit과 integration를 interchange할 수 있다는 정리였다.  

*Proof* (ii) 

\begin{align\*}
\ds M_X^{(m)}(t) &= \df{d^m}{dt^m}\left(\sum_{n=0}^{\inf}\df{t^n\mu_n}{n!}\right) \\\\&= \sumn\df{\mu_n}{n!}\df{d^m(t^n)}{dt^m} \\\\&= \sum_{n=m}^{\inf}\mu_n\df{t^{n-m}}{(n-m)!} \\\\&= \sum_{j=0}^{\inf} \mu_{m+j}\df{t^j}{j!}\;(j\equiv n-m)
\end{align\*}

<br>

**Example 1.15** *Find the mgf of the random variable $X$.*

\- $X\sim N(\mu,\si^2)$

\- $X\sim Gamma(\alpha,\beta)$

<br>

**Theorem 1.12** *Suppose $\\{X_i\\}$, $i=1,2,...$ is a sequence of random variables,  
each with mgf $M_{X_i}(t)$. Furthermore, suppose that*

\begin{align\*}
\underset{i\to\inf}{lim}M_{X_i}(t)=M_X(t)
\end{align\*}

*for all $t$ in a neighborhood of $0$, and $M_X(t)$ is an mgf. Then there is a **unique cdf** $F_X(x)$  
whose moments are determined by $M_X(t)$ and we have*

\begin{align\*}
\underset{i\to\inf}{lim}F_{X_i}(t)=F_X(t)
\end{align\*}

*for all $x$ where $F_X(x)$ is continuous. That is, convergence of mgfs to an mgf implies  
convergence of cdfs.*

> if mgfs of random variable are same, then they have an identical distribution.
> (증명은 어려움)

**Example 1.16** *Let $X$ follow a binomial distribution with $n$ and $p$, and  
$Y$ follow a Poisson distribution with $\lam=np$. The mgf of $X$ is computed as*

\begin{align\*}
\ds M_X(t) &= \sum_{x=0}^{n}e^{tx} {n \choose x} p^x (1-p)^{n-x} \\\\&= \sum_{x=0}^{n}{n \choose x} (pe^t)^x (1-p)^{n-x} \\\\&= [pe^t+(1-p)]^n\;(\t{for all $\|t\|<\e$)}​
\end{align\*}

> 이항정리: $(a+b)^n=\ds \sum_{x=0}^{n} {n\choose x}a^x\cdot b^{n-x}$

*and the mgf of $Y$ is computed as* 

\begin{align\*}
\ds M_Y(t) &= \sum_{y=0}^{\inf}e^{ty}\df{\lam^ye^{-\lam}}{y!} \\\\&= \sum_{y=0}^{\inf} \df{(\lam e^t)^ye^{-\lam e^t}}{y!}e^{\lam e^t}e^{-\lam} \\\\&= e^{\lam(e^t-1)}.
\end{align\*}

> pdf of $Pois(\lam e^t)=\df{(\lam e^t)^ye^{-\lam e^t}}{y!} $ for $y=0,1,2,...$

*As $n\to\inf$, the mgf of $X$ converges to*

\begin{align\*}
\limn M_X(t) &= \limn \left\\{ 1+\df{np(e^t-1)}{n} \right\\}^n \\\\ &= e^{\lam(e^t-1)} \\\\ &= M_Y(t)
\end{align\*}

*when $\lam(=np)$ remains fixed.*

> $\limn \left( 1+\df{a_n}{n} \right)^n=e^a$ when $a_n \to a$ 

<br>

**Theorem 1.13** *For any constants $a$ and $b$, the mgf of the random variable $aX+b$ is given by*

\begin{align\*}
M_{aX+b}(t)=e^{bt}M_X(at).
\end{align\*}

> $\because E[e^{t(aX+b)}]=E[e^{atX+bt}]=e^{bt}\cdot E[e^{taX}]=e^{bt}M_X(at).$

<br>

## **1.3.4 Characteristic function**

특성함수는 mgf와 유사하지만 조건 없이 복소수 공간에서 항상 존재하는 것이 장점이다.   
mgf는 0 근처의 $t$, $\|t\|<\e$  에서만 정의가 되었다.

1) Characteristic function of a random variable $X$ is defined as 

\begin{align\*}
\phi_X(t)=E\\{exp(itx)\\}=E\\{\t{cos $tX$ $+$ $i$ sin $tx$}\\}(\t{by Euler formula.})\;t\in\R,​
\end{align\*}

where $i=\sqrt{-1}$.

<br>

2) Main uses of characteristic function 

(i) Characterize <sub>규정</sub> the probability distribution of a given random variable.

(ii) Establish convergence in distribution of a sequence of random variables and  
to identify the limit distribution.

> always, if $\phi_X(t)=\phi_Y(t)\implies X\overset{D}{\equiv}Y.$

<br>

3) Comparison with MGF

\- MGF may not exist when the MGF is not finite.

\- Characteristic function always exists, because it is always bounded.

\begin{align\*}
\|\phi_X(t)\| &= [\\{E(\t{cos $tX$)}\\}^2+\\{E(\t{sin $tX$)}\\}^2]^{1/2}\\\\  &\leq [\\{E(\t{cos $tX$)}^2\\}+\\{E(\t{sin $tX$)}^2\\}]^{1/2}=1
\end{align\*}

> $\iff \|E\\{exp(itx)\\}\|\leq E\\{\|exp(itx)\|\\}$



***

## **Reference**







